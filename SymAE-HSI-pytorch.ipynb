{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f71d6d-ae92-49f8-9297-dd51880b9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c06b36-0a3d-40b0-a313-3e031b5e94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nband = 144\n",
    "nclass = 15\n",
    "nτ = 8\n",
    "nc = 64\n",
    "nd = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc26293-ab94-4bca-95b6-fe5361fc26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702a186-40f0-4f68-8ca9-f57d6d6780b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYMAE(nn.Module):\n",
    "    def __init__(self, nband, nc, nd, nτ):\n",
    "        super(SYMAE, self).__init__()\n",
    "        self.nband = nband\n",
    "        self.nc = nc\n",
    "        self.nd = nd\n",
    "        self.nτ = nτ\n",
    "\n",
    "        self.cenc = nn.Sequential(\n",
    "            nn.Linear(self.nband, 300), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(300, 300), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(300, 300), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(300, 150), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(150, nc), nn.LeakyReLU(0.5)\n",
    "        )\n",
    "\n",
    "        self.nenc = nn.Sequential(\n",
    "            nn.Linear(self.nband, 300), nn.LeakyReLU(0.5), nn.Dropout(0.25),\n",
    "            nn.Linear(300, 300), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(300, 300), nn.LeakyReLU(0.5), nn.Dropout(0.25),\n",
    "            nn.Linear(300, 150), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(150, nd), nn.LeakyReLU(0.5)\n",
    "        )\n",
    "\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(nc + nd, 150), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(150, 300), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(300, 300), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(300, 300), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(300, 300), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(300, 300), nn.LeakyReLU(0.5),\n",
    "            nn.Linear(300, self.nband)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.cenc(x)\n",
    "        c = c.mean(dim=1, keepdim=True).repeat(1, self.nτ, 1)\n",
    "        n = self.nenc(x)\n",
    "        n = nn.Dropout(0.5)(n)\n",
    "        x = torch.cat([c, n], dim=2)\n",
    "        x = self.dec(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c75140-a6d5-48b1-bee9-67ad3f8932e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SYMAE(nband,nc, nd, nτ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a1cac2-2df1-4268-8699-7bb523e541e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import h5py\n",
    "\n",
    "def load_mat(filename):\n",
    "    try:\n",
    "        return sio.loadmat(filename)\n",
    "    except NotImplementedError:\n",
    "        with h5py.File(filename, 'r') as file:\n",
    "            data = {}\n",
    "            for key in file.keys():\n",
    "                array = file[key][()]\n",
    "                if isinstance(array, np.ndarray):\n",
    "                    array = np.transpose(array, axes=range(array.ndim)[::-1])\n",
    "                data[key] = array\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff604e80-f9a8-41c5-a890-8e89515961e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_class_dict(HSI, label_mat):\n",
    "    class_dict = {}\n",
    "    for i in range(1, nclass+1): \n",
    "        class_dict[i] = []\n",
    "\n",
    "    # Get the indices of non-zero elements in the label matrix\n",
    "    nonzero_indices = np.nonzero(label_mat)\n",
    "\n",
    "    # Get the class labels for the non-zero indices\n",
    "    class_labels = label_mat[nonzero_indices]\n",
    "\n",
    "    # Get the corresponding pixels from the HSI data\n",
    "    pixels = HSI[nonzero_indices[0], nonzero_indices[1], :]\n",
    "\n",
    "    # Iterate over the class labels and append pixels to the corresponding list in the dictionary\n",
    "    for class_label, pixel in zip(class_labels, pixels):\n",
    "        class_dict[class_label].append(pixel)\n",
    "\n",
    "    # Convert the lists in the dictionary to NumPy arrays\n",
    "    for class_label in class_dict:\n",
    "        class_dict[class_label] = np.array(class_dict[class_label], dtype=np.float32)\n",
    "\n",
    "    return class_dict\n",
    "\n",
    "# Load the data using the load_mat function\n",
    "IN_PATH = ''\n",
    "scene = 'Houston.mat'# Change paths according to your file and system\n",
    "mat_data = load_mat(IN_PATH + scene)\n",
    "HSI = mat_data['input']\n",
    "HSI = (HSI - HSI.min())/(HSI.max() - HSI.min())\n",
    "train_coords = mat_data[\"TR\"]\n",
    "test_coords = mat_data[\"TE\"]\n",
    "full_gt = train_coords + test_coords\n",
    "\n",
    "# Create the dictionary of matrices\n",
    "class_dict = create_class_dict(HSI, train_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e676b-e6bb-42be-864e-55758b9ac3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HSIDataset(Dataset):\n",
    "    def __init__(self, class_dict, ntau, batches_per_epoch):\n",
    "        self.class_dict = class_dict\n",
    "        self.ntau = ntau\n",
    "        self.class_labels = list(class_dict.keys())\n",
    "        self.batches_per_epoch = batches_per_epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batches_per_epoch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Randomly select a class label\n",
    "        class_label = np.random.choice(self.class_labels)\n",
    "        class_pixels = self.class_dict[class_label]\n",
    "        \n",
    "        # Randomly select ntau pixels from the class\n",
    "        indices = np.random.choice(len(class_pixels), self.ntau, replace=True)\n",
    "        selected_pixels = class_pixels[indices]\n",
    "        \n",
    "        return torch.from_numpy(selected_pixels)\n",
    "\n",
    "def create_data_loader(class_dict, ntau, batch_size, samples_per_epoch, shuffle=True, num_workers=0):\n",
    "    dataset = HSIDataset(class_dict, ntau, samples_per_epoch)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    return data_loader\n",
    "\n",
    "# Assuming you have already created the class_dict using the create_class_dict function\n",
    "\n",
    "ntau = 8  # Number of pixels per class in each sample\n",
    "batch_size = 256  # Batch size for training\n",
    "batches_per_epoch = 2048  # Number of samples to generate per epoch\n",
    "datapoints_per_epoch = batches_per_epoch*batch_size\n",
    "num_workers = 4  # Number of worker processes for data loading\n",
    "\n",
    "# Create the data loader\n",
    "data_loader = create_data_loader(class_dict, ntau, batch_size, datapoints_per_epoch, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea99600-afc6-4755-b83e-058a0be36587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm  # Use tqdm specifically designed for Jupyter notebooks\n",
    "from IPython.display import clear_output  # Correct import for clear_output\n",
    "\n",
    "# Assuming you have defined the SYMAE model and the data_loader\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 3000\n",
    "\n",
    "# Initialize a list to store the training losses\n",
    "train_loss_store = []\n",
    "\n",
    "# Training loop with tqdm for epochs\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\", leave=True):\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Iterate over the data loader\n",
    "    for batch in data_loader:\n",
    "        # Move the batch to the device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Calculate the average training loss for the epoch\n",
    "    train_loss /= len(data_loader)\n",
    "    train_loss_store.append(train_loss)\n",
    "    \n",
    "    # Optionally clear the output of the tqdm bar to ensure no extra prints\n",
    "    if epoch % 10 == 0:  # Adjust or remove the conditional based on your preference\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    # Print the epoch and training loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.8f}\")\n",
    "\n",
    "# Switch to evaluation mode for any further operations on the model\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64078aba-6914-4948-813a-b8dfad71c7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b2f53-3c81-437a-a5b2-83815d37d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss_store, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8d7ef-bb73-4eaf-bff9-f9d0d18f049f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a17ffa-3b49-420c-864a-fa16642675bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b735f9e-3b3a-45b5-9b62-8302cc5e410c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ee63b-97a6-4aaf-89d8-7b33d07c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = create_class_dict(HSI, test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba1273-a3ba-409c-97ba-b2f70604f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = create_class_dict(HSI, train_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c8094-35f8-4037-bc45-371c48b0fcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeaa235-0dd2-4b7b-b4e5-4e21796960d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the classification model\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, ncc):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(ncc, 8 * 128)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.fc2 = nn.Linear(8 * 128, 4 * 128)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(4 * 128, 64)\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.fc4 = nn.Linear(64, 13)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.softmax(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Assuming you have the trained SYMAE model, the training data dictionary (train_dict),\n",
    "# and the test data dictionary (test_dict)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac516da-b333-407a-90ab-7f80e4bb8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the classification model\n",
    "cmodel = ClassificationModel(nc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa838f-ed30-43d2-8797-6c880e7dfd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e551e-b39e-4fb4-84be-75aeac187fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741415bb-c21f-4b08-8e1f-4aed184f59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cmodel.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f90d6-9646-4ba7-8aeb-7adb528cb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cmodel.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faaebd0-859e-455b-9cc4-28b12968263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all training pixels and labels\n",
    "train_pixels = []\n",
    "train_labels = []\n",
    "for class_label, pixels in train_dict.items():\n",
    "    train_pixels.append(pixels)\n",
    "    train_labels.extend([class_label] * len(pixels))\n",
    "\n",
    "train_pixels = np.concatenate(train_pixels, axis=0)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Move the training data to the device\n",
    "train_pixels_tensor = torch.from_numpy(train_pixels).float().to(device)\n",
    "train_labels_tensor = torch.from_numpy(train_labels).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130e294-8c9e-44d0-b988-083495817647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "cmodel.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    perm = torch.randperm(train_pixels_tensor.size(0))\n",
    "    train_pixels_tensor = train_pixels_tensor[perm]\n",
    "    train_labels_tensor = train_labels_tensor[perm]\n",
    "\n",
    "    # Iterate over batches\n",
    "    for i in range(0, train_pixels_tensor.size(0), batch_size):\n",
    "        # Get the batch\n",
    "        batch_pixels = train_pixels_tensor[i:i+batch_size]\n",
    "        batch_labels = train_labels_tensor[i:i+batch_size]\n",
    "\n",
    "        # Adjust the labels to 0-based indexing\n",
    "        batch_labels = batch_labels - 1\n",
    "\n",
    "        # Pass the batch pixels through the SYMAE model to get the latent representations\n",
    "        latent_repr = model.cenc(batch_pixels)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cmodel(latent_repr)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the training loss for every 10 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789a120-1d24-4a0a-85d6-96509ba15744",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a4875-faf3-4304-b2ca-efeb5ee94765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "test_pixels = []\n",
    "test_labels = []\n",
    "for class_label, pixels in test_dict.items():\n",
    "    test_pixels.append(pixels)\n",
    "    test_labels.extend([class_label] * len(pixels))\n",
    "\n",
    "test_pixels = np.concatenate(test_pixels, axis=0)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Adjust the test labels to 0-based indexing\n",
    "test_labels = test_labels - 1\n",
    "\n",
    "test_pixels_tensor = torch.from_numpy(test_pixels).float().to(device)\n",
    "test_labels_tensor = torch.from_numpy(test_labels).long().to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    latent_repr = model.cenc(test_pixels_tensor)\n",
    "    outputs = cmodel(latent_repr)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "predicted_cpu = predicted.cpu().numpy()\n",
    "test_labels_cpu = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(test_labels_cpu, predicted_cpu)\n",
    "precision = precision_score(test_labels_cpu, predicted_cpu, average='weighted')\n",
    "recall = recall_score(test_labels_cpu, predicted_cpu, average='weighted')\n",
    "f1 = f1_score(test_labels_cpu, predicted_cpu, average='weighted')\n",
    "\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764606e5-146e-454f-ae43-8a7249ecd97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc24ba-d175-41f1-9f14-62f78946789c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f1cec-f428-42ad-887b-5fab446be4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
